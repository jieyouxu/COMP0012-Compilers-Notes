\section{Syntactic Analysis}

The language used in \textit{Syntactic Analysis} is the \textit{Context-Free Grammar} (CFG). Sometimes \textit{Parsing Expression Grammar} (PEG) is also used, but PEG is not covered in this course.

\subsection{Context-Free Grammar}

\begin{definition}[Context-Free Grammar (CFG)]
    A \textit{Context-Free Grammar} (CFG) $G$ can be described by the 4-tuple
    \begin{equation}
        G = \langle
            \mathcal{N},
            \mathcal{T},
            S,
            \mathcal{R}
        \rangle
    \end{equation}
    
    Where
    \begin{enumerate}
        \item \textbf{Non-terminals}: $\mathcal{N}$ is the finite set of \textit{non-terminals} (uppercase by convention).
        \item \textbf{Terminals}: $\mathcal{T}$ is the finite set of \textit{terminals} (lowercase by convention).
        \item \textbf{Start Symbol}: $S \in \mathcal{T}$ is the \textit{start} symbol.
        \item \textbf{Production Rules}: $\mathcal{R} \coloneqq \mathcal{N} \to (\mathcal{N} \cup \mathcal{T})^\ast$ is the set of finite relations, termed \textit{productions} or \textit{rules} of the grammar.
    \end{enumerate}
\end{definition}

\begin{remark}
    By convention,
    \begin{itemize}
        \item Nonterminals are written in uppercase.
        \item Terminals are either punctuation characters or written in lowercase.
        \item Start symbol is the left-hand side non-terminal of the first product.
    \end{itemize}
\end{remark}

\begin{example}
    A \textit{production}
    \begin{equation*}
        X \to Y_1 \cdots Y_n
    \end{equation*}
    Means that the non-terminal $X$ can be \textit{replaced} by $Y_1 \cdots Y_n$. Equivalently, the production right-hand side $Y_1 \cdots Y_n$ is \textit{produced} by $X$.
\end{example}

\subsubsection{Language of a Context-Free Grammar}

\begin{definition}[Language of a Context-Free Grammar]
    Given context-free grammar $G$ with the start symbol $S$, the \textit{language} of $G$ is
    \begin{equation}
        L(G) \coloneqq \left\lbrace a_1 \cdots a_n \mid S \xrightarrow{\ast} a_1 \cdots a_n, a_i \in \mathcal{T} \right\rbrace
    \end{equation}
    
    That is, $L(G)$ is the set of all strings of \textit{terminals} for which the grammar $G$ can generate in zero or more steps.
\end{definition}

\subsubsection{Terminals}

\begin{definition}[Terminals]
    Characters in the input alphabet $\mathcal{T}$ are called \textit{terminals} because there does not exist any production rules which can replace them.
    
    That is, \textit{terminals} only appear in the \textit{right-hand side} of any production rule.
\end{definition}

\begin{remark}
    If terminals are generated by any replacement steps, then they are \textit{permanent}.
    
    \textit{Terminals} are often \textit{lexemes} in the language. For example, the terminal \texttt{void} is a keyword lexeme in the C language.
\end{remark}

\subsection{Right Regular Grammar}

\begin{definition}[Right-regular Grammar]
    A \textit{Right-regular Grammar} $G$ is the 4-tuple
    \begin{equation}
        G = \langle
            \mathcal{N},
            \mathcal{T},
            S,
            \mathcal{R}
        \rangle
    \end{equation}
    
    And also satisfying three additional \textit{constraints}; given $A, B \in \mathcal{N}$ and $a \in \mathcal{T}$, and given $B$ is \textit{right-regular},
    \begin{enumerate}
        \item $A \to a$
        \item $A \to aB$
        \item $A \to \epsilon$
    \end{enumerate}
\end{definition}

\begin{remark}
    Note that a \textit{Left-regular Grammar} has the above definition with only one difference, in the second constraint; given $A, B \in \mathcal{N}$ and $B$ is \textit{left-regular},
    \begin{equation}
        A \to Ba
    \end{equation}

    If left and right rules are mixed together, a linear grammar is generated which is context-free.
\end{remark}

\subsubsection{Derivations}

\begin{definition}[Derivation]
    A \textit{derivation} is a sequence of \textit{sential forms} resulting from repeated applications of some \textit{production rule} beginning from $S$, replacing \textit{non-terminals} with their respective \textit{productions}.
\end{definition}

\begin{example}
    For instance,
    \begin{align*}
        S &\to X_1 \cdots X_a \cdots X_b \cdots \\
          &\to X_1 \cdots Y_c \cdots X_b \cdots \\
          &\to \cdots \\
          &\to Y_1 \cdots Y_m \\
          &\to \cdots \\
          &\to \alpha_1 \cdots \alpha_n \\
    \end{align*}
    
    The intermediate strings with mixed terminals and non-terminals are the \textit{sentential forms}.
    
    The last derivation with $S \xRightarrow{\ast} \alpha_1 \cdots \alpha_n$ with each $\alpha_1, \dots, \alpha_n \in \mathcal{T}$ being terminals is called a \textit{sentence}.
\end{example}

\begin{definition}[Sentential Form]
    A \textit{sentential form} $\alpha$ refers to the \textit{start symbol} $S$ or any strings of \textit{terminals} and \textit{non-terminals} $(\mathcal{N} \cup \mathcal{T})^\ast$ that can be \textit{derived} from $S$.
    \begin{equation}
        \alpha = (\mathcal{N} \cup \mathcal{T})^\ast
    \end{equation}
\end{definition}

\begin{definition}[Sentence]
    A \textit{sentence} $\beta$ refers to a string of only \textit{terminals}, usually the last step in a sequence of \textit{derivations}.
    \begin{equation}
        \beta = \mathcal{T}^\ast
    \end{equation}
\end{definition}

\subsection{Parse Trees}

\begin{definition}[Parse Tree]
    A \textit{parse tree} illustrates a \textit{derivation}.
    \begin{enumerate}
        \item Start symbol $S$ is the \textit{root} of the tree.
        \item For production $X \to Y_1 \cdots Y_n$, create a \textit{node} with \textit{key} $X$ and \textit{children} $Y_1, \dots, Y_n$.
    \end{enumerate}
    
    \begin{figure}[H]
        \centering
        \begin{forest}
            for tree={
                draw=black,
                edge={->},
                edge from parent/.style={draw=black}
            }
            [$S$
                [$X$
                    [$Y_1$]
                    [$\dots$]
                    [$Y_n$]
                ]
            ]
        \end{forest}
        \caption{Generic Parse Tree. Note root $S$ is the start symbol and $X$ is an interior node of $S$ with children $Y_1, \dots, Y_n$.}
        \label{fig:generic-parse-tree}
    \end{figure}
\end{definition}

\begin{remark}
    A \textit{parse tree} corresponds with a \textit{Context-Free Grammar} by having
    \begin{itemize}
        \item Terminals as \textit{leaves}.
        \item Non-terminals as \textit{interior nodes}.
    \end{itemize}
    
    \begin{figure}[H]
        \centering
        \begin{forest}
            for tree={
                draw=black,
                edge={->},
                edge from parent/.style={draw=black}
            }
            [$S$
                [$\dots$
                    [$\mathcal{N}$
                        [$\mathcal{T}$]
                        [$\dots$]
                        [$\mathcal{T}$]
                    ]
                ]
            ]
        \end{forest}
        \caption{Parse tree correspondence with CFG}
        \label{fig:parse-tree-and-cfg}
    \end{figure}
    
    \begin{itemize}
        \item An \textit{in-order traversal} of the \textit{parse tree} yields the original input string.
        \item The \textit{parse tree} contains the \textit{association} of \textit{operations} for which the input string does not.
    \end{itemize}
\end{remark}

\subsection{Left-most and Right-most Derivations}

\begin{definition}[Left-most Derivation]
    The \textit{left-most derivation} occurs when the \textit{left-most non-terminal} is always expanded.
\end{definition}

\begin{definition}[Right-most Derivation]
    The \textit{right-most derivation} occurs when the \textit{right-most non-terminal} is always expanded.
\end{definition}

\begin{example}
    For a very simple grammar for addition and subtraction expressions
    \begin{equation*}
        \begin{matrix}
            S &\to  & E + E \\
              &\mid & E - E \\
              &\mid & E \\
            E &\to  & a \\
              &\mid & b \\
        \end{matrix}
    \end{equation*}
    
    This grammar is \textit{ambiguous} because two parse trees can be produced for the input string
    \begin{equation*}
        a + b - a
    \end{equation*}
    
    \begin{figure}[H]
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
            \begin{forest}
                for tree={
                    draw=black,
                    edge={->},
                    if n children=0{tier=terminal}{},
                    s sep=10pt,
                },
                [$A$
                    [$A$
                        [$a$]
                    ]
                    [$+$]
                    [$A$
                        [$A$
                            [$b$]
                        ]
                        [$-$]
                        [$A$
                            [$a$]
                        ]
                    ]
                ]
            \end{forest}
            \caption{Left-most derivation}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \centering
            \begin{forest}
                for tree={
                    draw=black,
                    edge={->},
                    if n children=0{tier=terminal}{},
                    s sep=10pt,
                },
                [$A$
                    [$A$
                        [$A$
                            [$a$]
                        ]
                        [$+$]
                        [$A$
                            [$b$]
                        ]
                    ]
                    [$-$]
                    [$A$
                        [$a$]
                    ]
                ]
            \end{forest}
            \caption{Right-most derivation}
        \end{subfigure}
        \caption{Ambiguous grammar with multiple valid parse trees}
        \label{fig:left-most-right-most-ambiguous-grammar}
    \end{figure}
    
    When the \textit{left-most} and \textit{right-most} derivations both produce the \textit{same} parse tree, then the grammar is \textit{unambiguous}.
\end{example}

\subsection{Chomsky Hierarchy}

\begin{definition}[Chomsky Hierarchy]
    The \textit{Chomsky Hierarchy} classifies different languages in increasing expressive power.
    
    \begin{figure}[H]
        \centering
        \begin{tabularx}{\textwidth}{@{} X X X X @{}}
            \toprule
            Expressiveness & Grammar & Production Constraint & Automata \\
            \midrule
            $\uparrow$ & Universal & $\alpha \to \beta$ & Turing \par Machine \\
            \phantom{} & Context-Sensitive & $\alpha A \beta \to \alpha \delta \beta$ & Linear \par Bounded \par Automata \\
            \phantom{} & Context-Free & $A \to \alpha$ & Push-Down \par Automata \\
            $\downarrow$ & Regular (right) & $A \to a \mid aB \mid \epsilon$ & Deterministic \par Finite \par Automata \\
            \bottomrule
        \end{tabularx}
        \caption{Chomsky Hierarchy}
        \label{fig:chomsky-hierarachy}
    \end{figure}
\end{definition}

\subsection{Solutions to Resolve Ambiguity}

\begin{enumerate}
    \item \textbf{Hacking}: Rewrite the grammar to enforce the precedence.
    \begin{example}
        Simple grammar solution
        \begin{equation*}
            \begin{matrix}
                E &\to  & E + E \\
                  &\mid & E * E \\
                  &\mid & ( E ) \\
                  &\mid & id \\
             \end{matrix}
        \end{equation*}
        converting to 
        \begin{equation*}
            \begin{matrix}
                E &\to  & E' + E \\
                  &\mid & E' \\
                  \\
                E'&\to  & (E) * E'\\
                  &\mid & (E) \\
                  &\mid & id * E' \\
                  &\mid & id \\
             \end{matrix}
        \end{equation*}
    \end{example}
    \item \textbf{Unambiguous annotations}: \\
    Using natural grammar but along with forced annotations (for example, precedence).
    
    \begin{example} A simple example\\
    
        \begin{BVerbatim}
        left  + 
        right * 
        \end{BVerbatim}
    \end{example}
    
    Where \\
    
    \begin{BVerbatim}
    %left  -->  left associative
    %right -->  right associative
    \end{BVerbatim}
        
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \node [phantomblock] (pre) {Precedence};
            \node [phantomblock, right=0.3em of pre] (rm) {};
            \node [block, above=0.5em of rm] (rt) {Last rule};
            \node [block, below=0.5em of rm] (rb) {First rule};
            \draw [->] (rb) -- (rt);
        \end{tikzpicture}
    \end{figure}
\end{enumerate}

\subsection{Top-Down Parsing}

\begin{definition}[Top-Down Parsing]
    Given an input string consisting of terminals
    \begin{equation*}
        t_1\ t_2\ \cdots\ t_n
    \end{equation*}
    
    Then its \textit{parse tree} is constructed
    \begin{itemize}
        \item From \textit{top} to \textit{bottom}.
        \item From \textit{left} to \textit{right}.
    \end{itemize}
\end{definition}

\subsubsection{Recursive Descent Parsing}

\begin{definition}[Parsing]
    \textit{Parsing} aims to find the parse tree for a string in grammar $G$
    \begin{equation}
        t_1\ t_2\ \cdots\ t_n
    \end{equation}
\end{definition}

\begin{definition}[Recursive Descent Parsing]
    \textit{Recursive Descent Parsing} tries to build the parse tree starting from the start symbol $S$ and trying all productions exhaustively.
    \begin{itemize}
        \item The \textit{fringe} of the parse tree is
            \begin{equation}
                t_1\ t_2\ \cdots\ t_k\ A\ \cdots
            \end{equation}
        \item The parser needs to \textit{backtrack} if the fringe does not match the $k$-length prefix of the string.
        \item Try all productions for $A$.
        \begin{itemize}
            \item If there is a production $A \to BC$
            \item Then the new fringe is
                \begin{equation}
                    t_1\ t_2\ \cdots\ t_k\ B\ C\ t_{k+1}\ \dots\ t_n
                \end{equation}
        \end{itemize}
        \item Terminate when there are no more non-terminals remaining.
    \end{itemize}
\end{definition}

\subsubsection{Limitations of Recursive Descent}

\textit{Recursive Descent} does \textit{not} work when the grammar is \textit{left-recursive}.

\begin{definition}[Left-recursive Grammar]
    A \textit{left-recursive grammar} has a non-terminal $A$ which, for zero or more derivations, derives
    \begin{equation}
        S \to^{+} S\ \alpha
    \end{equation}
    For some $\alpha \in (\mathcal{N} \cup \mathcal{T})^\ast$. Recursive descent parsing \textit{diverges} on \textit{left-recursive} grammars.
\end{definition}

\begin{example}
    Given a production
    \begin{equation}
        S \to S\ a
    \end{equation}
    
    Then it is possible to keep on replacing $S$ in the right-hand side with the production yielding no fringe to match (because the start can always be the non-terminal):
    \begin{align*}
        S &\to S\ a \\
        \phantom{} &\to S\ a\ a \\
        \phantom{} &\to S\ a\ a\ a\ \cdots \\
    \end{align*}
\end{example}

\subsubsection{Elimination of Left Recursion}

\begin{definition}[Left Recursion Elimination]
    Given a grammar $G$ which contains a \textit{left-recursive} production of non-terminal $A$
    \begin{equation}
        \begin{aligned}
            A \to  &\ A\ \alpha_1 \\
              \mid &\ \cdots \\
              \mid &\ A\ \alpha_n \\
              \mid &\ \beta_1 \\
              \mid &\ \cdots \\
              \mid &\ \beta_m
        \end{aligned}
    \end{equation}
    Where $\alpha_i \in (\mathcal{N} \cup \mathcal{T})^+$ and $\beta_i \in ((\mathcal{N} - \set{A}) \cup \mathcal{T})^+$.
    
    Then this rule can be converted into a \textit{right-recursive} rule by introducing a new non-terminal $A\prime$ and rewriting the original rule to give
    \begin{equation}
        \begin{aligned}
        A \to  &\ \beta_1\ A\prime \\
          \mid &\ \cdots \\
          \mid &\ \beta_m\ A\prime \\
        A\prime \to  &\ \alpha_1\ A\prime \\
                \mid &\ \cdots \\
                \mid &\ \alpha_n\ A\prime \\
                \mid &\ \epsilon
        \end{aligned}
    \end{equation}
\end{definition}

\begin{definition}[Indirect Left Recursion Elimination]
    Given some production rules
    \begin{equation}
        \begin{aligned}
            S \to &\ A\ \alpha \mid \beta \\
            A \to &\ S \beta
        \end{aligned}
    \end{equation}
    
    Then $S$ is \textit{left recursive} since
    \begin{equation}
        S \to^+ S\ \beta\ \alpha
    \end{equation}
    
    This indirect left recursion can be eliminated by back-substituting $A$ into $S$ to give
    \begin{equation}
        S \to S\ \beta\ \alpha \mid \delta
    \end{equation}
    
    Then perform the previous rewrite technique.
\end{definition}

\begin{definition}[Nullable Non-terminal]
    A non-terminal $N$ is \textit{nullable} iff $N \to^+ \epsilon$.
\end{definition}

\begin{definition}[$\epsilon$-masked Left Recursion Elimination]
    Given some production with the form
    \begin{equation}
        \begin{aligned}
            S \to &\ Y\ S \mid q \\
            Y \to &\ A\ x\ B \mid C\ c \mid c\ d \mid \epsilon
        \end{aligned}
    \end{equation}
    
    Since $Y \to^+ \epsilon$ is \textit{nullabe}, then $S$ is in fact \textit{left-recursive}. This left recursion can be eliminated by performing back-substitution, similar to the indirect recursion case, then perform the previous rewrite technique.
    
    Note that should the back-substitution step generate any rules of the form
    \begin{equation}
        Y \to Y
    \end{equation}
    Then that production can be simply discarded (it has no effect).
    
    Also note that such back-substitution may cause an exponential growth in the number of production rules.
\end{definition}

\begin{definition}[Generic Left Recursion Elimination]
    With each left recursive type considered, it is possible to give a generic left recursion elimination procedure.
    \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Procedure{EliminateLeftRecusion}{$G$}
                \Repeat
                    \State{\Call{EliminateUnreachableRules}{$G$}}
                    \State{\Call{EliminateUselessProductions}{$G$}}
                    \State{\Call{EliminateDirectLeftRecursionViaRewrite}{$G$}}
                    \State{\Call{EliminateIndirectLeftRecursionViaBackSubstitution}{$G$}}
                    \State{\Call{EliminateEpsilonMaskedLeftRecursionViaBackSubstitution}{$G$}}
                \Until{\Call{NoEliminationApplies}{\null}}
            \EndProcedure
        \end{algorithmic}
        \caption{Generic Left Recursion Elimination}
        \label{prog:generic-left-recursion-elimination}
    \end{algorithm}
\end{definition}

\subsubsection{Predictive Parsers}

\begin{definition}[Predictive Parser]
    Similar to \textit{recursive descent} parsers but with \textit{lookahead(s)} to choose the correct production. I.e. for some grammar $G$, when looking at
    \begin{equation}
        t_1\ t_2\ \cdots\ t_k\ B\ C\ t_{k+1}\ \cdots\ t_n
    \end{equation}
    
    The terminals $t_{k+1}\ \cdots\ t_n$ may be considered to choose a production. 
\end{definition}

\begin{definition}[$LL(k)$ Grammars]
    \textit{Predictive} parsers accept $LL(k)$ grammars, where $LL(k)$ denotes \textit{left-to-right} input scan, \textit{leftmost} derivation with $k$ tokens of \textit{lookahead.} Usually $LL(1)$ grammars are used in practice.
\end{definition}

\subsubsection{Parse Tables}

\begin{definition}[Parse Tables]
    Given a context-free grammar $G$, then its \textit{Parse Table} is a 2D table with the dimensions:
    \begin{enumerate}
        \item Current non-terminal production.
        \item Next $k$ lookahead tokens.
    \end{enumerate}
    
    Then each table entry $T[N, a]$ where $N \in \mathcal{N}, a \in \mathcal{T}$ contains a subset of productions which has the non-terminal head in the first dimension, i.e.
    \begin{equation}
        T[N, a] = \set{(N \to \alpha), (N \to \beta), \cdots}
    \end{equation}
\end{definition}

\begin{definition}[$LL(1)$ Parse Table]
    $LL(1)$ grammars imply that given a pair of non-terminal and lookahead token $(N, a)$ where $N \in \mathcal{N}, a \in \mathcal{T}$, there exists only \textit{one} production which can lead to a successful parse.
    
    That is, a $LL(1)$ parse table has two dimensions:
    \begin{enumerate}
        \item Current non-terminal.
        \item Next lookahead token $k = 1$.
    \end{enumerate}
    
    The restriction by $LL(1)$ grammars means that each table entry is restricted to contain only a single production, i.e.
    \begin{equation}
        T[N, a] = \set{ (N \to \alpha) }
    \end{equation}
    
    Where
    \begin{equation}
        \left\lvert T[N, a] \right\rvert \equiv 1
    \end{equation}
\end{definition}

\begin{remark}
    Note that trash states may be implicitly denoted by empty cells, which do not violate the single-production-per-cell constraint for $LL(1)$ grammars.
\end{remark}

\subsubsection{Left Factoring}

\begin{definition}[Converting to $LL(1)$ Grammar]
    A grammar may have to be \textit{left-factored} into a $LL(1)$ grammar before it is able to be parsed via predictive parsing.
\end{definition}

\begin{example}
    Two productions with the same lookahead token for the same non-terminal cause the grammar to be beyond $LL(1)$, e.g.
    \begin{equation}
        \begin{aligned}
            T \to  &\ int \\
              \mid &\ int * T \\
        \end{aligned}
    \end{equation}
    
    Since both productions begin with $int$, it is impossible for the $LL(1)$ predictive parse to choose which production is the correct one based on a single lookahead token.
\end{example}

\begin{definition}[Left Factoring]
    \textit{Left Factoring} is the process to rewrite a grammar so that each production of each non-terminal has a unique prefix.
    
    This is analogous to factoring out a common prefix, i.e.
    \begin{equation}
        ax + ay = a * (x + y)
    \end{equation}
    
    Given a non-terminal $N$ that needs to be left factored, and some $A, B \in (\mathcal{N} \cup \mathcal{T})^+$, supposing that $N$ has the form
    \begin{equation}
        \begin{aligned}
            N \to  &\ A\ B_1 \\
              \mid &\ A\ B_2 \\
              \mid &\ \cdots \\
              \mid &\ A\ B_n \\
              \mid &\ \langle \text{rules not prefixed with\ } A \rangle
        \end{aligned}
    \end{equation}
    
    Then it can be left-factored by introducting a new non-terminal $N\prime$ and rewriting $N$ to the form
    \begin{equation}
        \begin{aligned}
            N \to  &\ A\ A\prime \\
              \mid &\ \langle \text{rules not prefixed with\ } A \rangle \\
            A\prime \to  &\ B_1 \\
                    \mid &\ B_2 \\
                    \mid &\ \cdots \\
                    \mid &\ B_n
        \end{aligned}
    \end{equation}
    
    Note that any $B_i$ can be $\epsilon$.
\end{definition}

\subsubsection{LL(1) Parsing}

\begin{definition}[$LL(1)$ Parse Actions]
    Given a $LL(1)$ parse table $T$
    \begin{enumerate}
        \item If $T[N, a] = N \to \beta$:
        \begin{itemize}
            \item $N$ is to be replaced by or expanded to $\beta$.
        \end{itemize}
        \item If $T[N, a] = \epsilon$:
        \begin{itemize}
            \item $N$ is to be discarded.
        \end{itemize}
    \end{enumerate}
\end{definition}

\begin{definition}[Building $LL(1)$ Parse Table]
    Given the parse is in some state considering
    \begin{equation}
        S \to^\ast \beta\ A\ \gamma
    \end{equation}
    Where $A$ is the non-terminal being considered and $b$ is the lookahead token.
    
    Then there exists two possibilities for the position of $b$:
    \begin{enumerate}
        \item $b \in \First{A}$: $b$ belongs to some expansion of $A$, that is, if $b$ can start a string derived from $\alpha$ from the production
        \begin{equation}
            A \to \alpha
        \end{equation}
        \item $b \in \Follow{A}$: $b$ does not belong to any expansion of $A$.
        \begin{itemize}
            \item $A \to^\ast \epsilon$ and $b \in \First{\gamma}$, in $S \to^\ast \beta\ A\ b\ \omega$ supposing $\gamma \to^\ast b\ \omega$.
            \item Note that any of $A \to \alpha$ can be used if $\alpha \to^\ast \epsilon$, i.e. $\epsilon \in \First{A}$.
        \end{itemize}
    \end{enumerate}
\end{definition}

\begin{definition}[First Set]
    Given a grammar $G$ and a non-terminal $X \in \mathcal{N}$, then the \textit{First} set of $X$ is defined as
    \begin{equation}
        \First{X} \coloneqq \set{t \in \mathcal{T} \given X \to^+ t\ \alpha} \cup \set{\epsilon \given X \to^+ \epsilon}
    \end{equation}
    
    Where $\alpha \in (\mathcal{N} \cup \mathcal{T})^\ast$.
\end{definition}

\begin{definition}[Computing First Sets]
    Given a grammar $G = \langle \mathcal{N}, \mathcal{T}, \mathcal{R}, S \rangle$, with $\mathcal{T}_\epsilon \coloneq \mathcal{T} \cup \set{\epsilon}$, then the algorithm to compute the first set of its non-terminal $X \in \mathcal{N}$ is given as
    \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Procedure{ComputeFirstSet}{$\langle \mathcal{N}, \mathcal{T}, \mathcal{R}, S \rangle$, $X \in \mathcal{N}$}
                \State $\Forall t \in \mathcal{T}_\epsilon \colon  \First{t} \gets \set{t}$
                \State $\First{X} \gets \emptyset$
                \ForAll{productions $P = X \to A_1\ \cdots\ A_n \in \mathcal{R}$}
                    \ForEach{$A_i$}
                        \State $\First{X} \gets \First{X} \cup (\First{A_i} - \set{\epsilon})$
                        \If{$\epsilon \not\in \First{A_i}$}
                            \State \Call{NextProduction}{\null}
                        \EndIf
                    \EndFor
                    \State $\First{X} \gets \First{X} \cup \set{\epsilon}$ \Comment{Only if all $A_i \in P$ are nullable}
                \EndFor
                \State \Return{$\First{X}$}
            \EndProcedure
        \end{algorithmic}
        \caption{Computing First Set}
        \label{prog:compute-first-set}
    \end{algorithm}
\end{definition}

\begin{definition}[Follow Sets]
    Given some grammar $G$ and non-terminals $\mathcal{N}$, then the \textit{Follow} set of non-terminal $X$ is given by
    \begin{equation}
        \begin{split}
            \Follow{X} \coloneqq &\ \set{ t \given Y \to \alpha\ X\ \beta \land t \in (\First{\beta} - \set{\epsilon}) } 
            \cup \\ &\ \set{ t \given \beta \to^\ast \epsilon \land t \in \Follow{Y} }
        \end{split}
    \end{equation}
    
    Note that the non-terminal $X$ appearing on the right-hand side of the production rule of non-terminal $Y$. It is also required that $\Follow{X}$ is the minimum set, if in the process of calculating $\Follow{X}$ that $\Follow{X}$ appears on the right-hand side, i.e. if there is an intermediate step
    \begin{equation}
        \Follow{X} = \cdots \cup \Follow{X} \cup \cdots
    \end{equation}
    
    Then the $\Follow{X}$ on the right-hand side is simply absorbed and has no effect.
\end{definition}

\begin{definition}[Computing Follow Sets]
    Given a grammar $G = \langle \mathcal{N}, \mathcal{T}, \mathcal{R}, S \rangle$, with end-of-input symbol $\$$, then the algorithm to compute the follow set of its non-terminal $X \in \mathcal{N}$ is given as
    \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Procedure{ComputeFollowSet}{$\langle \mathcal{N}, \mathcal{T}, \mathcal{R}, S \rangle$, $X \in \mathcal{N}$}
            \ForAll{non-terminals $N \in \mathcal{N}$}
                \State $\First{N}$ \gets \Call{ComputeFirstSet}{$\langle \mathcal{N}, \mathcal{T}, \mathcal{R}, S \rangle$, $N$}
            \EndFor
            \State $\Follow{X} \gets \emptyset$
            \State $\Follow{S} \gets \set{\$}$ \Comment{Starting non-terminal $S$}
            \ForAll{productions $P = Y \to \cdots\ X\ A_1\ \cdots\ A_n \in \mathcal{R}$}
                \ForEach{$A_i$}
                    \State $\Follow{X} \gets \Follow{X} \cup (\First{A_i} - \set{\epsilon})$
                    \If{$\epsilon \not\in \First{A_i}$}
                        \State \Call{NextProduction}{\null}
                    \EndIf
                \EndFor
                \State $\Follow{X} \gets \Follow{X} \cup \Follow{Y}$ \Comment{Iff all $A_i \in P$ is nullable}
            \EndFor
            \EndProcedure
        \end{algorithmic}
        \caption{Computing Follow Set}
        \label{prog:compute-follow-set}
    \end{algorithm}
    
    It should be noted that
    \begin{itemize}
        \item $\epsilon \not\in \Follow{X}$
    \end{itemize}
\end{definition}

\begin{definition}[Building $LL(1)$ Parse Table Algorithm]
    To construct a $LL(1)$ parse table $T$ for a given grammar $G$
    \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Procedure{BuildLL1ParseTable}{$G$}
                \ForEach{production $A \to \alpha$}
                    \ForEach{terminal $b \in \First{\alpha} - \set{\epsilon}$}
                        \State $T[A, b] \gets \alpha$
                    \EndFor
                    \If{$\alpha \to^\ast \epsilon$}
                        \ForEach{$b \in \Follow{A}$}
                            \State $T[A, b] \gets \epsilon$
                        \EndFor
                    \EndIf
                \EndFor
            \EndProcedure
        \end{algorithmic}
        \caption{Building $LL(1)$ parse table}
        \label{prog:build-ll(1)-parse-table}
    \end{algorithm}
\end{definition}

\begin{remark}
    A $LL(1)$ parse table has exactly one production per table cell. If there are multiple productions per table cell, then the grammar is $LL(1)$ because of any of the following reasons:
    \begin{itemize}
        \item $G$ is \textit{ambiguous}
        \item $G$ is \textit{left recursive}
        \item $G$ is not \textit{left factored}
    \end{itemize}
\end{remark}

\begin{definition}[Using $LL(1)$ Parse Table]
    A $LL(1)$ parse utilizes the $LL(1)$ parse table similar to that in recursive descent, but with differences in that
    \begin{itemize}
        \item For each \textit{non-terminal} $N$
        \item Consider the lookahead token $a$
        \item Choose production at
            \begin{equation}
                T[N, a]
            \end{equation}
    \end{itemize}
    
    The $LL(1)$ parser uses a \textit{stack} to record pending non-terminals
    \begin{itemize}
        \item \textit{Reject} if an error state is encountered.
        \item \textit{Accept} if \textit{end-of-input} is reached.
    \end{itemize}
\end{definition}

\begin{definition}[$LL(1)$ Parsing Algorithm]
    Given input stream of characters, with $\langle H\ rest \rangle$ denoting the top of the stack and the rest of the stack respectively, then the $LL(1)$ parsing algorithm is given by
    \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Procedure{ParseWithLL1}{$InputTokenStream$}
                \State $I \gets InputTokenStream$
                \State $stack \gets \langle S\ \$ \rangle$
                \State $head \gets 0$
                \Repeat
                    \Switch{$\langle \tau\ rest \rangle$}
                        \Comment{$\tau$ is the top of stack}
                        \Case{$\tau \in \mathcal{N}$}
                            \Comment{$\tau$ is a non-terminal}
                            \If{$T[\tau, I[head]] \equiv Y_1\ \cdots\ Y_n$}
                                \State $stack \gets \langle Y_1\ \cdots\ Y_n, rest \rangle$
                            \Else
                                \State \Call{ReportError}{$\tau$}
                            \EndIf
                            \State $\textbf{break}$
                        \EndCase
                        \Case{$\tau \in \mathcal{T}$}
                            \Comment{$\tau$ is a terminal}
                            \If{$\tau \equiv I[head]$}
                                \State $stack \gets \langle rest \rangle$
                                \State $head{+}{+}$
                            \Else
                                \State \Call{ReportError}{$\tau$}
                            \EndIf
                            \State $\textbf{break}$
                        \EndCase
                    \EndSwitch
                \Until{$stack \equiv \langle \$ \rangle$}
            \EndProcedure
        \end{algorithmic}
        \caption{$LL(1)$ Parsing Algorithm}
        \label{prog:ll1-parsing-algorithm}
    \end{algorithm}
    
    Note that the right-hand side of $\tau$, $Y_1\ \cdots\ Y_n$, is the production for which the $head$ selects.
\end{definition}

\subsection{Bottom-Up Parsing}
